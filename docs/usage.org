#+TITLE:     Usage
#+AUTHOR:    Kynon J. Benjamin, Ph.D
#+EMAIL:     kynonjade.benjamin@libd.org
#+LANGUAGE:  en
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="http://gongzhitaao.org/orgcss/org.css"/>
#+PROPERTY:  header-args: :dir /dcs04/lieber/statsgen/jbenjami/tutorials/eqtl_analysis_tutorial
#+PROPERTY:  header-args:R :cache yes :exports both :session *R* :eval never-export
#+PROPERTY:  header-args:python :session *Python* :cache yes :exports both :eval never-export
#+PROPERTY:  header-args:sh :cache yes :exports both :eval never-export
#+OPTIONS:   H:3 num:nil toc:3 \n:nil @:t ::t |:t ^:{} -:t f:t *:t TeX:t LaTeX:t skip:t d:(HIDE) tags:not-in-toc
#+STARTUP:   align fold nodlcheck hidestars oddeven lognotestate
#+TAGS:      Write(w) Update(u) Fix(f) Check(c) noexport(n)

* General workflow
Using =rfmix-reader= is as simple as [[https://pandas-plink.readthedocs.io/en/latest/usage.html][=pandas-plink=]] but extended to
RFMix, FLARE, and Haptools simulations. The sections below show how to
prepare input data, read each supported format, understand the returned
objects, and visualize the results.

** Preparing input data
The canonical pipeline starts with RFMix's =*.fb.tsv= files. Convert them
to binary chunks with =create_binaries= to minimize later I/O.

#+begin_src python :results output
  from rfmix_reader import create_binaries

  prefix_path = "../examples/two_populations/out/"
  create_binaries(prefix_path)
#+end_src

#+RESULTS[20f284c23771c5bf268a8cb4a468752edf5b0410]:
: Created binary files at: ./binary_files
: Converting fb files to binary!
100% 3/3 [05:03<00:00, 101.27s/it]
As of =v0.1.20= you can also trigger the conversion from the CLI.
** Preparing reference stores for phasing
The phasing path in =read_rfmix= requires per-chromosome VCF-Zarr stores and
sample annotations. Use the =prepare-reference= CLI to convert bgzipped,
indexed VCF/BCF files into the ``<chrom>.zarr`` layout expected by phasing.

#+begin_src shell
  prepare-reference -h
#+end_src

#+begin_note
#+begin_example
usage: prepare-reference [-h] [--chunk-length CHUNK_LENGTH]
                         [--samples-chunk-size SAMPLES_CHUNK_SIZE]
                         [--worker-processes WORKER_PROCESSES]
                         [--verbose | --no-verbose] [--version]
                         output_dir vcf_paths [vcf_paths ...]

Convert one or more bgzipped reference VCF/BCF files into Zarr stores.

positional arguments:
  output_dir            Directory where the Zarr outputs will be written.
  vcf_paths             Paths to reference VCF/BCF files (bgzipped and indexed).

options:
  -h, --help            show this help message and exit
  --chunk-length CHUNK_LENGTH
                        Genomic chunk size for the output Zarr stores (default: 100000).
  --samples-chunk-size SAMPLES_CHUNK_SIZE
                        Chunk size for samples in the output Zarr stores (default: library chosen).
  --worker-processes WORKER_PROCESSES
                        Number of worker processes to use for conversion (default: 0, use library default).
  --verbose, --no-verbose
                        Print progress messages (default: enabled).
  --version             Show the version of the program and exit.
#+end_example
#+end_note

End-to-end setup::

  # sample annotations: sample_id<TAB>group (no header)
  cat > sample_annotations.tsv <<'EOF'
  NA19700	AFR
  NA19701	AFR
  NA20847	EUR
  EOF

  # generate per-chromosome VCF-Zarr stores
  prepare-reference refs/ 1kg_chr20.vcf.gz 1kg_chr21.vcf.gz \
    --chunk-length 50000 --samples-chunk-size 512

  # phase per chromosome after reading RFMix outputs
  phase_rfmix_chromosome_to_zarr(
      file_prefix="../examples/two_populations/out/",
      ref_zarr_root="refs",
      sample_annot_path="sample_annotations.tsv",
      output_path="./phased_chr21.zarr",
      chrom="21",
  )
** Reading RFMix output
=read_rfmix= combines the loci metadata, chromosome-level global ancestry
files, and local ancestry chunks. If =binary_dir= is omitted the function
will transparently build the binaries in the background.
  from rfmix_reader import read_rfmix

  loci_df, g_anc, admix = read_rfmix(
      "../examples/two_populations/out/",
      binary_dir="../examples/two_populations/out/binary_files",
  )

Pass ``chrom`` if you only want to read one chromosome; phasing is now handled
by :func:`phase_rfmix_chromosome_to_zarr` and
:func:`merge_phased_zarrs` in ``rfmix_reader.processing.phase``.

#+begin_src python :results output
  from rfmix_reader.processing.phase import phase_rfmix_chromosome_to_zarr
  from rfmix_reader.processing.phase import merge_phased_zarrs

  ds = phase_rfmix_chromosome_to_zarr(
      file_prefix="../examples/two_populations/out/",
      ref_zarr_root="./reference_zarr",
      sample_annot_path="sample_annotations.tsv",
      output_path="./phased_chr21.zarr",
      chrom="21",
  )

  merged = merge_phased_zarrs(
      ["./phased_chr21.zarr", "./phased_chr22.zarr"],
      output_path="./phased_all.zarr",
  )
#+end_src

From the command line, the same merge can be performed with:

#+begin_src bash :results none
  merge-phased-zarrs ./phased_all.zarr ./phased_chr21.zarr ./phased_chr22.zarr
#+end_src

** Reading FLARE output
=read_flare= mirrors the =read_rfmix= interface but expects FLARE's
=*.anc.vcf.gz= and =global.anc.gz= pairs. The resulting =(loci_df, g_anc,
admix)= tuple therefore drops directly into the rest of the workflow.

#+begin_src python :results output
  from rfmix_reader import read_flare

  loci_df, g_anc, admix = read_flare("/path/to/flare/prefix/")
#+end_src

** Reading Haptools simulations
=read_simu= parses BGZF-compressed VCFs produced by
=haptools simgenotype --pop_field=. It infers the chromosome list,
computes the per-sample global ancestry matrix, and builds the same
output tuple as the other loaders.

#+begin_src python :results output
  from rfmix_reader import read_simu
  loci_df, g_anc, admix = read_simu("/path/to/simulations/")
#+end_src
* Understanding the outputs
Every reader returns =loci_df=, =g_anc=, and =admix= to keep downstream
code identical.
** =loci_df=
  loci_df.shape
  loci_df.head()
       chromosome  physical_position  i
0           chr20              60137  0
1           chr20              60291  1
2           chr20              60340  2
3           chr20              60440  3
4           chr20              60823  4

The =i= column mirrors =pandas_plink= and simplifies joins with other
variant-level resources.
** =g_anc=
=g_anc= (previously called =rf_q=) stores the global ancestry proportions
per individual and chromosome.
  g_anc.shape
  g_anc.head()
You can still extract sample IDs or the set of populations for later use.
  sample_ids = g_anc.sample_id.unique().to_arrow()
  pops = g_anc.drop(["sample_id", "chrom"], axis=1).columns.values
** =admix=
=admix= exposes the local ancestry array as a =dask.array=, matching the
BED semantics from =pandas_plink= so that only the requested chunks are
materialized into memory.
#+begin_src python :results value verbatim
#+RESULTS:
#+begin_src python :results value
#+RESULTS:
#+begin_example
[[2 2 2 ... 0 0 0]
 [2 2 1 ... 0 0 1]
 [1 2 1 ... 0 0 0]
 ...
 [1 1 2 ... 0 0 0]
 [2 2 2 ... 1 1 1]
 [2 2 1 ... 1 0 1]]
#+end_example
Column ordering follows the population-major convention (all individuals
for population 1, then all individuals for population 2, etc.). Generate
column labels with a list comprehension when needed.
#+RESULTS:
* Loci imputation
Imputation now lives in =rfmix_reader.processing.imputation= (re-exported as
=interpolate_array=). It fills missing RFMix loci on an arbitrary variant grid
and writes a Zarr store (=<zarr_outdir>/local-ancestry.zarr=) with shape
=(variants, samples, ancestries).

*Inputs*
- =variant_loci_df=: pandas DataFrame with =chrom=, =pos=, and the source RFMix
  row index in column =i=. Any row with =i= =NaN= is treated as a missing locus
  to be interpolated. Sort by genomic coordinate and include =pos= if you want
  base-pair interpolation.
- =admix=: the local ancestry array from ~read_rfmix~ (shape
  =(loci, samples, ancestries)=).
- =zarr_outdir=: directory where =local-ancestry.zarr= will be created.

*Key options*
- =interpolation=: ="linear"= (default), ="nearest"=, or ="stepwise"=.
- =use_bp_positions=True= interpolates along =variant_loci_df['pos']= instead of
  treating loci as evenly spaced indices.
- =chunk_size=/batch_size= tune how many rows are materialized per write or
  interpolation step.

*Workflow example*
#+begin_src python :results none
  from pathlib import Path
  import pandas as pd
  from rfmix_reader import interpolate_array, read_rfmix

  loci_df, _, admix = read_rfmix("two_pops/out/", binary_dir="./binary_files")

  variant_df = pd.read_parquet("genotypes/variants.parquet")
  variant_df = variant_df.drop_duplicates(subset=["chrom", "pos"]).sort_values("pos")
  variant_loci_df = (
      variant_df.merge(loci_df.to_pandas(), on=["chrom", "pos"], how="outer", indicator=True)
                 .loc[:, ["chrom", "pos", "i", "_merge"]]
  )

  z = interpolate_array(
      variant_loci_df,
      admix,
      zarr_outdir=Path("./imputed_local_ancestry"),
      interpolation="nearest",
      use_bp_positions=True,
      chunk_size=50_000,
  )
#+end_src

~interpolate_array~ will leverage CUDA (via =cupy=) when present; otherwise it
falls back to NumPy. Interpolation operates on diploid-summed trajectories and
preserves the original ancestry dimension.

* Visualization
=rfmix-reader= bundles several plotting/export helpers so you can inspect
results without additional reshaping.

** Global summaries
Use =plot_global_ancestry= for per-individual stacked bars and
=plot_ancestry_by_chromosome= for chromosome-level boxplots. Both accept
the =save_path= and =save_multi_format= options exposed by the plotting
utilities.

#+begin_src python :results none
  from rfmix_reader import (
      plot_global_ancestry,
      plot_ancestry_by_chromosome,
  )

  plot_global_ancestry(g_anc, dpi=300, bbox_inches="tight")
  plot_ancestry_by_chromosome(g_anc, dpi=300, bbox_inches="tight")
#+end_src

** Export for TAGORE and BED-based tools
=generate_tagore_bed= converts =(loci_df, g_anc, admix)= to an annotated
BED-style table that matches TAGORE's expectations. Use it whenever you
need external visualization or browser-ready tracks.

#+begin_src python :results value
  from rfmix_reader import generate_tagore_bed, plot_local_ancestry_tagore

  tagore_df = generate_tagore_bed(loci_df, g_anc, admix, sample_num=0)
  tagore_df.head()

  plot_local_ancestry_tagore(
      tagore_df,
      prefix="local-ancestry-sample0",
      build="hg38",
      oformat="png",
      force=True,
  )
#+end_src
#+end_src

=plot_local_ancestry_tagore= writes a =.svg= plus a converted =.png= or =.pdf=
(via CairoSVG). Set =force=True= to regenerate the SVG; PNG/PDF conversions
overwrite existing files regardless of =force=.

#+RESULTS[943d0f4206518c373fa852ab000059693e2b2897]:
: ['AFR' 'EUR']

*** =admix=
=admix= is the convert RFMix results from the =*.fb.tsv= files.
Here, we add the alleles and re-subset the data so that the
first population is first (all samples) followed by the next, and
the next. This means instead of 0 and 1, you can get 0, 1, or 3.

#+begin_src python :results value
  admix
#+end_src

#+RESULTS[786d091553720e67cc5780ad7bbd2265492be434]:
: dask.array<concatenate, shape=(646287, 1000), dtype=float32, chunksize=(1024, 256), chunktype=numpy.ndarray>

To reduce memory consumption, this large data is held in a
dask array, exactly like =pandas_plink= BED data.

#+begin_src python :results value verbatim
  admix.compute()
#+end_src

#+RESULTS[070fc2065a660e8042230bf7713804fdb124fbba]:
: [[2 2 2 ... 0 0 0]
:  [2 2 1 ... 0 0 1]
:  [1 2 1 ... 0 0 0]
:  ...
:  [1 1 2 ... 0 0 0]
:  [2 2 2 ... 1 1 1]
:  [2 2 1 ... 1 0 1]]

#+begin_src python :results value verbatim
  admix.shape
#+end_src

#+RESULTS[19574afcca5d5cbc89e58eb226076e4ed3afeab7]:
: (646287, 1000)

The rows are the same as the =loci= data, in the sample order.

#+begin_src python :results value verbatim
  loci.shape
#+end_src

#+RESULTS[217b70fa31fcce528d45f44213a25d1722e1309b]:
: (646287, 3)

The rows are the total samples x number of populations. This
is in a specific order. All samples are grouped by population
instead of by the sample.

#+begin_src python :results value verbatim
  col_names = [f"{sample}_{pop}" for pop in pops for sample in sample_ids]
  len(col_names)
#+end_src

#+RESULTS[6d3b0a823d116490484f2500f47ebbb03fcd208c]:
: 1000

#+begin_src python :results value verbatim
  col_names[0:4]
#+end_src

#+RESULTS[c8ca5d8c680865988858e9cafb571adceb27970d]:
: ['Sample_1_AFR', 'Sample_2_AFR', 'Sample_3_AFR', 'Sample_4_AFR']

#+begin_src python :results value verbatim
  col_names[500:504]
#+end_src

#+RESULTS[9889ae17959e0911178a53e41e70d58d7ce11224]:
: ['Sample_1_EUR', 'Sample_2_EUR', 'Sample_3_EUR', 'Sample_4_EUR']

This is the correct order for the admix array data.

* Loci Imputation
Imputing local ancestry loci information to genotype variant locations improves
integration of the local ancestry information with genotype data. As such, we also
provide the =interpolate_array= function to efficiently interpolate missing values
when local ancestry loci information is converted to more variable genotype variant
locations. It leverages the power of [[https://zarr.readthedocs.io/en/stable/index.html][=Zarr=]] arrays, making it suitable for handling
substantial datasets while managing memory usage effectively.

*Note*: Following imputation, =variant_df= will include genomic positions for
both local ancestry and genotype data.

#+BEGIN_SRC python :results silent
  def _load_genotypes(plink_prefix_path):
      from tensorqtl import pgen
      pgr = pgen.PgenReader(plink_prefix_path)
      variant_df = pgr.variant_df
      variant_df.loc[:, "chrom"] = "chr" + variant_df.chrom
      return pgr.load_genotypes(), variant_df

  def _load_admix(prefix_path, binary_dir):
      from rfmix_reader import read_rfmix
      return read_rfmix(prefix_path, binary_dir=binary_dir)

#+END_SRC

#+BEGIN_SRC python :results silent
  from rfmix_reader import interpolate_array
  basename = "/projects/b1213/large_projects/brain_coloc_app/input"
  # Local ancestry
  prefix_path = f"{basename}/local_ancestry_rfmix/_m/"
  binary_dir = f"{basename}/local_ancestry_rfmix/_m/binary_files/"
  loci, _, admix = _load_admix(prefix_path, binary_dir)
  loci.rename(columns={"chromosome": "chrom",
		       "physical_position": "pos"},
	      inplace=True)
  # Variant data
  plink_prefix = f"{basename}/genotypes/TOPMed_LIBD"
  _, variant_df = _load_genotypes(plink_prefix)
  variant_df = variant_df.drop_duplicates(subset=["chrom", "pos"],
					  keep='first')
  # Keep all locations for more accurate imputation
  variant_loci_df = variant_df.merge(loci.to_pandas(), on=["chrom", "pos"],
				     how="outer", indicator=True)\
			      .loc[:, ["chrom", "pos", "i", "_merge"]]
  data_path = f"{basename}/local_ancestry_rfmix/_m"
  z = interpolate_array(variant_loci_df, admix, data_path)
#+END_SRC
